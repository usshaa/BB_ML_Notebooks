{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_all.html\"> Click Linear Regression Simulation\n",
        "</a>"
      ],
      "metadata": {
        "id": "7kBaihZiW1Eg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkKDCX4gRPTl"
      },
      "source": [
        "**Linear regression** is a fundamental statistical technique used to model and analyze the relationships between a dependent variable and one or more independent variables. It's a powerful tool in data science and statistics for prediction and inference.\n",
        "\n",
        "\n",
        "### 1. **Basics of Linear Regression**\n",
        "Linear regression aims to model the relationship between two variables by fitting a linear equation to the observed data. The equation of a simple linear regression line is:\n",
        "\n",
        "y=Œ≤0+Œ≤1x+œµ\n",
        "\n",
        "y is the dependent variable (the outcome we are trying to predict or explain).\n",
        "\n",
        "x is the independent variable (the predictor or explanatory variable).\n",
        "\n",
        "Œ≤0 is the intercept (the value of y when x=0).\n",
        "\n",
        "Œ≤1 is the slope (the change in y for a one-unit change in x).\n",
        "\n",
        "œµ is the error term (the difference between the observed and predicted values of ùë¶\n",
        "y).\n",
        "\n",
        "### 2. **Assumptions of Linear Regression**\n",
        "For linear regression to provide reliable results, several key assumptions must be met:\n",
        "\n",
        "1. **Linearity**: The relationship between the independent and dependent variables should be linear.\n",
        "2. **Independence**: The observations should be independent of each other.\n",
        "3. **Homoscedasticity**: The residuals (errors) should have constant variance at all levels of the independent variable.\n",
        "4. **Normality**: The residuals should be approximately normally distributed.\n",
        "5. **Multicollinearity**: Predictors are not correlated with each other\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "### 3. **Fitting a Linear Regression Model**\n",
        "The goal is to find the best-fitting line by minimizing the sum of the squared differences between the observed values and the values predicted by the line. This method is called **Ordinary Least Squares (OLS)**. The formula for the slope (\\( \\beta_1 \\)) and intercept (\\( \\beta_0 \\)) are:\n",
        "\n",
        "slope = np.sum((X - x_mean) * (y - y_mean)) / np.sum((X - x_mean) ** 2)\n",
        "\n",
        "intercept = y_mean - slope * x_mean\n",
        "\n",
        "\n",
        "### 4. **Evaluating the Model**\n",
        "Several metrics can be used to evaluate the goodness of fit for a linear regression model:\n",
        "\n",
        "- **R-squared (\\( R^2 \\))**: This measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1, with higher values indicating a better fit.\n",
        "  \n",
        "- **Adjusted R-squared**: This adjusts \\( R^2 \\) for the number of predictors in the model, providing a more accurate measure for multiple regression models.\n",
        "\n",
        "- **Standard Error**: This measures the average distance that the observed values fall from the regression line.\n",
        "\n",
        "- **p-values**: Used to test the hypothesis that a coefficient is different from zero. Low p-values (typically < 0.05) indicate that a predictor is significantly contributing to the model.\n",
        "\n",
        "### 5. **Multiple Linear Regression**\n",
        "When there are multiple independent variables, the model extends to:\n",
        "\n",
        "y=Œ≤0+Œ≤1x1+Œ≤2x2+‚ãØ+Œ≤pxp+œµ\n",
        "\n",
        "y is the dependent variable (the outcome we are trying to predict or explain).\n",
        "\n",
        "x1,x2,‚Ä¶,xp are the independent variables (the predictors or explanatory variables).\n",
        "\n",
        "ùõΩ0 is the intercept (the value of ùë¶ when all predictors are zero).\n",
        "\n",
        "Œ≤1,Œ≤2,‚Ä¶,Œ≤p are the coefficients (the change in ùë¶ for a one-unit change in each predictor, holding all other predictors constant).\n",
        "\n",
        "œµ is the error term (the difference between the observed and predicted values of y).\n",
        "\n",
        "### 6. **Diagnosing Problems**\n",
        "Diagnosing potential problems in a linear regression model involves:\n",
        "\n",
        "- **Residual Plots**: Plotting residuals to check for patterns (indicating non-linearity, non-constant variance).\n",
        "- **Normal Probability Plots**: Checking for normality of residuals.\n",
        "- **Variance Inflation Factor (VIF)**: Assessing multicollinearity (when independent variables are highly correlated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHngnikvRPTs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}